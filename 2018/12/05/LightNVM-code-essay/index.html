<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <title>LightNVM 代码分析总结 | Gnekiah&#39;s Serenice</title>
  <meta name="description" content="Just write something casually.">
  <meta name="keywords" content="">
  <meta name="HandheldFriendly" content="True">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="内核版本：Linux-4.19  Lightnvm 与 pblkLightnvm 与 pblk 的关系，类似于 linux 块层与 IO 调度器之间的关系。即在 lightnvm 中可以有多种 FTL 的实现，这里 pblk 就是一种 FTL 的实现。Lightnvm 子系统在支持PPA接口的块设备的基础上进行初始化。该模块使内核能够通过内部 nvm_dev 数据结构和 sysfs 等来暴露设备的">
<meta property="og:type" content="article">
<meta property="og:title" content="LightNVM 代码分析总结">
<meta property="og:url" content="http://spiral.xxiong.me/2018/12/05/LightNVM-code-essay/index.html">
<meta property="og:site_name" content="Gnekiah&#39;s Serenice">
<meta property="og:description" content="内核版本：Linux-4.19  Lightnvm 与 pblkLightnvm 与 pblk 的关系，类似于 linux 块层与 IO 调度器之间的关系。即在 lightnvm 中可以有多种 FTL 的实现，这里 pblk 就是一种 FTL 的实现。Lightnvm 子系统在支持PPA接口的块设备的基础上进行初始化。该模块使内核能够通过内部 nvm_dev 数据结构和 sysfs 等来暴露设备的">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://blog.xxiong.me/usr/uploads/2018/12/1939970917.jpg">
<meta property="og:image" content="http://blog.xxiong.me/usr/uploads/2018/12/4174733462.jpg">
<meta property="og:image" content="http://blog.xxiong.me/usr/uploads/2018/12/3526174751.jpg">
<meta property="og:image" content="http://blog.xxiong.me/usr/uploads/2018/12/3945780743.jpg">
<meta property="og:image" content="http://blog.xxiong.me/usr/uploads/2018/12/896036978.jpg">
<meta property="og:image" content="http://blog.xxiong.me/usr/uploads/2018/12/1745448200.jpg">
<meta property="og:image" content="http://blog.xxiong.me/usr/uploads/2018/12/1279183542.jpg">
<meta property="og:image" content="http://blog.xxiong.me/usr/uploads/2018/12/3129676206.jpg">
<meta property="og:updated_time" content="2018-12-05T03:23:48.240Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LightNVM 代码分析总结">
<meta name="twitter:description" content="内核版本：Linux-4.19  Lightnvm 与 pblkLightnvm 与 pblk 的关系，类似于 linux 块层与 IO 调度器之间的关系。即在 lightnvm 中可以有多种 FTL 的实现，这里 pblk 就是一种 FTL 的实现。Lightnvm 子系统在支持PPA接口的块设备的基础上进行初始化。该模块使内核能够通过内部 nvm_dev 数据结构和 sysfs 等来暴露设备的">
<meta name="twitter:image" content="http://blog.xxiong.me/usr/uploads/2018/12/1939970917.jpg">
  
  
    <link rel="icon" href="http://www.xxiong.me/favicon.png">
  

	<script src="https://use.typekit.net/eyf3hir.js"></script>
  <script>try{Typekit.load({ async: false });}catch(e){}</script>
  <link rel="stylesheet" href="/style.css">
  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>
  

</head>
</html>
<body>
  
  <div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>

  <script>setLoadingBarProgress(20)</script>
  
  <div id="site-wrapper">
    
    <header id="header">
	<div id="header-wrapper" class="clearfix">
		<a id="logo" href="/">
			<img src="/images/logo.png">
			<span id="site-desc">
			  很随便的，写点儿东西。
      </span>
		</a>
		<button id="site-nav-switch">
	    <span class="icon icon-menu"></span>
	  </button>
	</div>
	<aside id="site-menu">
  	<nav>
  		
        <a href="/" class="nav-index nav">
          首页
        </a>
      
        <a href="/archives" class="nav-archives nav">
          目录
        </a>
      
        <a href="http://blog.xxiong.me/" class="nav-blog nav">
          博客
        </a>
      
        <a href="http://blog.xxiong.me/about.html" class="nav-about nav">
          关于
        </a>
      
        <a href="http://www.xxiong.me" class="nav-home nav">
          主页
        </a>
      
    </nav>
	</aside>
</header>
    <script>setLoadingBarProgress(40);</script>
    
    <main id="main" role="main">
      <article id="post-LightNVM-code-essay" class="post article white-box article-type-post" itemscope="" itemprop="blogPost">
	<h2 class="title">
  	<a href="/2018/12/05/LightNVM-code-essay/">
    	LightNVM 代码分析总结
    </a>
  </h2>
	<time>
	  Dec 5, 2018
	</time>
	<section class="content">
  	<div class="article-entry" itemprop="articleBody">
    	<p>内核版本：Linux-4.19</p>
<hr>
<h2 id="Lightnvm-与-pblk"><a href="#Lightnvm-与-pblk" class="headerlink" title="Lightnvm 与 pblk"></a>Lightnvm 与 pblk</h2><p>Lightnvm 与 pblk 的关系，类似于 linux 块层与 IO 调度器之间的关系。即在 lightnvm 中可以有多种 FTL 的实现，这里 pblk 就是一种 FTL 的实现。Lightnvm 子系统在支持PPA接口的块设备的基础上进行初始化。该模块使内核能够通过内部 nvm_dev 数据结构和 sysfs 等来暴露设备的几何结构信息。通过这种方式，FTL 和用户空间的应用程序可以在使用前就了解到设备的底层信息。此外 Lightnvm 子系统还有一个最重要的功能——管理 target 的划分以及指定用于管理 target 的 FTL。</p>
<hr>
<h2 id="Lightnvm-的三层结构"><a href="#Lightnvm-的三层结构" class="headerlink" title="Lightnvm 的三层结构"></a>Lightnvm 的三层结构</h2><p>如下图所示，Lightnvm 被分为三个部分 ③②①。自上而下分别为 FTL、Lightnvm 子系统以及设备驱动程序。Lightnvm 子系统通过将 FTL 提供出来的 make_rq 函数指针替换到块层的 blk_queue_make_request，从而实现直接处理 bio 而不需要经过 IO 调度器。</p>
<p>当一个 bio 从文件系统发出来后，首先进入 FTL 模块，对应下图中的 ③；在 FTL 中处理完毕后，如果要与设备进行交互，则 FTL 必须要将请求下发到设备。由于 FTL 不知道底层设备类型，故也无法确定设备驱动程序所定义的数据格式，同时也为了确保 Lightnvm 能够对多种不同类型的设备都兼容，因此 Lightnvm 与具体设备驱动之间采取了一个中间件，叫做 Lightnvm 驱动相关层，其功能是将 Lightnvm 所定义的命令格式转换成具体设备驱动程序的命令格式，再提交给对应的驱动程序。由此可知，如果某个设备要使用 Lightnvm 模块，则该设备对应的驱动程序必须要实现上面提到的中间件。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/1939970917.jpg" alt="20181130202355.jpg"><br>!!!<br></center><br>!!!</p>
<p>Lightnvm 与具体设备驱动的中间件如下图示，Lightnvm 有一套通用的命令格式和请求格式，中间层只起到格式转换的作用。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/4174733462.jpg" alt="20181130202356.jpg"><br>!!!<br></center><br>!!!</p>
<p>使用 Lightnvm 的设备驱动程序使内核其他模块能够通过 PPA I/O 接口直接访问设备。设备驱动程序将设备作为传统的 Linux 块设备公开到用户空间，允许应用程序通过 ioctls 与设备进行交互。PPA 地址格式定义见下图：<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/3526174751.jpg" alt="20181130202357.jpg"><br>!!!<br></center><br>!!!</p>
<hr>
<h2 id="pblk-FTL-实现-的源码分析"><a href="#pblk-FTL-实现-的源码分析" class="headerlink" title="pblk (FTL 实现)的源码分析"></a>pblk (FTL 实现)的源码分析</h2><p>pblk 是对 Lightnvm 中负责具体 FTL 功能的一种实现，在旧版本中还有 rrpc，但是在最新的内核 4.19 版本中，已经移除了 rrpc 的模块，现在只有 pblk 这一种实现。从管理范围的范畴来说，pblk 是负责管理 target 的，一个设备可以切分成多个 target，而每个 target 可以使用不同的 FTL 的具体实现来进行管理。抽象出 target 这一方式，使内核空间模块或用户空间应用程序能够通过高级 I/O 接口（例如由 pblk 提供的块 I/O 接口的标准接口）访问设备，或由自定义 target 提供的为应用程序定制的接口来进行访问。其中，每一个 target 就是一块物理存储设备的抽象，每个 target 可以单独使用一种类型的 FTL 来管理，彼此间独立。pblk 主要的职责是以下几点：</p>
<ul>
<li>提供主机侧的写缓冲机制</li>
<li>实现从主机逻辑地址到设备物理地址的转换</li>
<li>处理本该由设备进行的垃圾回收</li>
</ul>
<hr>
<h2 id="模块之间的衔接"><a href="#模块之间的衔接" class="headerlink" title="模块之间的衔接"></a>模块之间的衔接</h2><p>从源码的角度分析，将 Lightnvm 插入原生的驱动程序与块层之间，需要解决两个交互点——块层与 Lightnvm 之间的衔接以及 Lightnvm 与驱动程序之间的衔接。这里我们将两个交互点整理成 2 个问题：</p>
<ol>
<li>在何处向请求队列注册 make request function 函数</li>
<li>在何处向请求队列注册 request function 函数</li>
</ol>
<p>其中注册 make request 函数是为了将能够直接处理文件系统中的 bio；注册 request 函数则是为了能够将请求发往具体驱动程序。下面分别对每个问题进行源码分析。</p>
<h4 id="向请求队列注册-pblk-make-rq"><a href="#向请求队列注册-pblk-make-rq" class="headerlink" title="向请求队列注册 pblk_make_rq"></a>向请求队列注册 pblk_make_rq</h4><p>操作系统为每个块设备维护一个 request queue。当一个支持 Lightnvm 的 nvme 设备被识别到后，lightnvm 模块将会为 nvme device driver 创建 target（类比于磁盘的分区），一个 Lightnvm 设备可以划分多个 target（要求划分的 target 的起始 channel 地址对齐，channel 结束地址没有要求），划分 target 的基本单位为 lun，并且不同的 target 可以采用不同的 FTL 的实现（例如 target 1 使用 pblk 管理，target 2 使用 rrpc 管理）。</p>
<p>这里创建 target 是通过调用函数 nvm_ctl_ioctl 实现的。在该函数中又间接调用了 nvm_create_tgt，其中有一步重要操作如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tqueue = blk_alloc_queue_node(GFP_KERNEL, dev-&gt;q-&gt;node);</span><br><span class="line">blk_queue_make_request(tqueue, tt-&gt;make_rq);</span><br></pre></td></tr></table></figure></p>
<p>通过这两段代码，将 pblk 模块的入口函数 pblk_make_rq（即 make request function）插入到内核为该 pblk 对应的 nvme device 分区维护的请求队列中。</p>
<h4 id="当-IO-请求下来时，如何调用-pblk-make-rq"><a href="#当-IO-请求下来时，如何调用-pblk-make-rq" class="headerlink" title="当 IO 请求下来时，如何调用 pblk_make_rq"></a>当 IO 请求下来时，如何调用 pblk_make_rq</h4><p>当 IO 请求从 block layer 下来后，执行下面的步骤将请求传给 Lightnvm 层。</p>
<ol>
<li>当一个请求下来后，首先执行 bio_alloc() 分配一个新的 bio 并初始化 bio 描述符；</li>
<li>bio 初始化完毕后，内核调用 generic_make_request() 函数；</li>
<li>在该函数中，调用 bdev_get_queue() 获取与请求的块设备相关的请求队列 rq；</li>
<li>之后调用 rq-&gt;make_request_fn() 将 bio 请求插入请求队列 rq 中；</li>
</ol>
<p>第 4 个步骤的 make_request_fn()（即 pblk_make_rq）在 target 初始化的时候已经插入到该请求队列中，因此调用这一步后就进入 lightnvm 模块中。lightnvm 模块与设备驱动相关的代码位于路径 /drivers/nvme/host/lightnvm.c 中。</p>
<h4 id="NVMe-向请求队列中注册-nvme-queue-rq"><a href="#NVMe-向请求队列中注册-nvme-queue-rq" class="headerlink" title="NVMe 向请求队列中注册 nvme_queue_rq"></a>NVMe 向请求队列中注册 nvme_queue_rq</h4><p>nvme 为 namespace 初始化 request queue 的操作：</p>
<ul>
<li><p>入口函数，其中包含下面这一步操作，用于初始化一个 request queue</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvme_alloc_ns() &#123;</span><br><span class="line">    ns-&gt;<span class="built_in">queue</span> = blk_mq_init_queue(ctrl-&gt;tagset);</span><br></pre></td></tr></table></figure>
</li>
<li><p>在该函数 (blk_mq_init_queue) 中调用了另一个函数如下，向 request queue 中插入 nvme 的操作函数：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blk_mq_init_allocated_queue() &#123;</span><br><span class="line">    q-&gt;mq_ops = <span class="built_in">set</span>-&gt;ops;</span><br></pre></td></tr></table></figure>
</li>
<li><p>其中 nvme 模块的 pci.c 文件中定义了操作入口：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">blk_mq_ops</span> <span class="title">nvme_mq_ops</span> = &#123;</span></span><br><span class="line">    .queue_rq     = nvme_queue_rq,</span><br><span class="line">    .init_request = nvme_init_request,</span><br><span class="line">    .......</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="当-lightnvm-处理完后，如何调用-nvme-queue-rq"><a href="#当-lightnvm-处理完后，如何调用-nvme-queue-rq" class="headerlink" title="当 lightnvm 处理完后，如何调用 nvme_queue_rq"></a>当 lightnvm 处理完后，如何调用 nvme_queue_rq</h4><p>当 Lightnvm 中与驱动相关的逻辑执行完成后（从 lightnvm request 生成特定驱动的命令），Lightnvm 的驱动相关层（nvme 中位于 drivers/nvme/host/lightnvm.c）将会调用 blk_execute_rq_nowait() 函数将请求交由通用块层处理。该函数部分代码如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">blk_execute_rq_nowait() &#123;</span><br><span class="line">    <span class="keyword">if</span> (q-&gt;mq_ops) &#123;</span><br><span class="line">        blk_mq_sched_insert_request(rq, at_head, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br></pre></td></tr></table></figure></p>
<p>在 blk_mq_sched_insert_request() 函数中有如下调用关系：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">blk_mq_sched_insert_request() &#123;</span><br><span class="line">    blk_mq_run_hw_queue(hctx, async);</span><br><span class="line">blk_mq_run_hw_queue() &#123;</span><br><span class="line">    __blk_mq_delay_run_hw_queue(hctx, async, <span class="number">0</span>);</span><br><span class="line">__blk_mq_delay_run_hw_queue() &#123;</span><br><span class="line">    __blk_mq_run_hw_queue(hctx);</span><br><span class="line">__blk_mq_run_hw_queue() &#123;</span><br><span class="line">    blk_mq_sched_dispatch_requests(hctx);</span><br><span class="line">blk_mq_sched_dispatch_requests() &#123;</span><br><span class="line">    blk_mq_dispatch_rq_list(q, &amp;rq_list);</span><br><span class="line">blk_mq_dispatch_rq_list() &#123;</span><br><span class="line">    <span class="comment">// 这里调用了nvme模块中的nvme_queue_rq</span></span><br><span class="line">    ret = q-&gt;mq_ops-&gt;queue_rq(hctx, &amp;bd);</span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="read"><a href="#read" class="headerlink" title="read"></a>read</h2><p>pblk 的读写过程见下图示，从整体的角度看，读请求和写请求都要首先对 L2P 表进行操作，之后再操作 write buffer。下面分别对 read 和 write 的过程进行详解。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/3945780743.jpg" alt="20181130202358.jpg"><br>!!!<br></center><br>!!!<br>read 操作大致可分为三步：</p>
<ol>
<li>从 L2P table 查找物理地址；</li>
<li>从 buffer 中查找该地址，若查找 cache 命中，则从 buffer 中读取；</li>
<li>如果查找 buffer 未命中，则构造读请求并从设备读取数据。</li>
</ol>
<p>如果只有部分请求命中，则要将未命中的请求重新构造成新的请求，再从设备读取。在部分命中的这种情况下，当从设备中读取数据后，需要将 buffer 中的命中的那部分数据与从设备中读取的未命中的数据进行合并，再返回给文件系统。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/896036978.jpg" alt="20181130202359.jpg"><br>!!!<br></center><br>!!!</p>
<hr>
<h2 id="write"><a href="#write" class="headerlink" title="write"></a>write</h2><p>write 操作分为两个部分：</p>
<ol>
<li>将文件系统下发的数据写入到 buffer；</li>
<li>write thread 将数据从 buffer 写入到设备。</li>
</ol>
<p>在整个写入的操作中，对 buffer 的操作可以分为两类角色：多个生产者和一个消费者。</p>
<ul>
<li>所有的写操作都会先将数据写入到 buffer 中；</li>
<li>然后由 write thread 将数据写入设备中。下面是这两个部分的具体操作流程。</li>
</ul>
<ol>
<li>写入 buffer 的主要操作如下：<ul>
<li>判断能否向 buffer 写入请求数据；</li>
<li>将数据写入到 buffer 中；</li>
<li>写入数据后，更新 L2P table；</li>
<li>判断是否需要唤醒 write thread。</li>
</ul>
</li>
<li>从 buffer 写入设备的主要操作：<ul>
<li>计算要写回的 entries 数量；</li>
<li>将 entries 添加到 bio 并构造 request；</li>
<li>向设备提交请求。</li>
</ul>
</li>
</ol>
<p>下面的流程图展示了向 buffer 中写数据的操作过程。当 write 请求到来时，首先要判断有没有 buffer 足够空间用来进行本次的写请求，如果空间不够，需要触发 write thread，将 buffer 中的数据写回设备中。如果空间足够，就将数据写入到 buffer 中，其中 buffer 中的单位是 entry，大小是 4KB。最后再更新 L2P table。这里在 end io 之前需要判断是否需要唤醒 write thread 将 buffer 中的数据写入到设备。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/1745448200.jpg" alt="20181130202360.jpg"><br>!!!<br></center><br>!!!</p>
<p>write thread 被唤醒的条件有两个：</p>
<ol>
<li>被定时器触发；</li>
<li>buffer 中需要写回到设备的数据量达到了阈值。</li>
</ol>
<p>当 write thread 被唤醒后，首先计算 buffer 中需要同步的 entries 的总数，这些是需要写回设备的数据单元。之后将 entries 中的数据添加到 bio，用于向设备发送写请求。这里需要注意，write thread 不需要更新 L2P table，因为这个操作在前半部分的 write to buffer 中已经完成了。</p>
<hr>
<h2 id="discard"><a href="#discard" class="headerlink" title="discard"></a>discard</h2><p>discard 的作用是使请求的数据无效化。discard 是针对 L2P table 的操作，只需要将 L2P table 的逻辑地址对应的物理地址设为 empty 就实现了将目标数据无效化的操作。涉及到 discard 的操作如下： </p>
<ol>
<li>当 discard 请求发送到来时：</li>
<li>首先查找 L2P table，找到对应的物理地址；</li>
<li>之后查找 write buffer；</li>
<li>如果 cache hit，则直接更新 L2P，将请求的逻辑地址对应的物理地址标记为无效；</li>
<li>如果 cache miss，则获取请求页地址所在的 line，将其标记为不可用（确保在该地址被无效化之前不会被访问），之后更新 L2P table。<br>!!!<br><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/1279183542.jpg" alt="20181130202361.jpg"><br>!!!<br></center><br>!!!</li>
</ol>
<hr>
<h2 id="GC"><a href="#GC" class="headerlink" title="GC"></a>GC</h2><p>GC 能够充分将存储单元利用起来。GC thread 被唤醒的条件有：</p>
<ol>
<li>定时器唤醒；</li>
<li>write thread 主动唤醒 GC thread。</li>
</ol>
<p>在 GC thread 被唤醒后，只遍历每个 line，并初始化每个 line 的工作队列。如上图所示，gc full list 中保存的是 lines，只有全部 full 的 line 才会加入 gc full list。停止 gc 的条件是 free blocks 数量达到预设的阈值。GC 是由 line 管理的，内核遍历由 line 中的工作队列组成的工作队列链表，对每一个 line，分别执行 GC 操作。在当前版本的 lightnvm 中，GC 的操作是简单的将数据读出来保存到 write buffer 中。关于擦除块的操作在 write thread 中。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/3129676206.jpg" alt="20181130202362.jpg"><br>!!!<br></center><br>!!!</p>
<hr>
<h2 id="Buffer-管理"><a href="#Buffer-管理" class="headerlink" title="Buffer 管理"></a>Buffer 管理</h2><p>Buffer 的相关数据结构如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">pblk_rb_entry</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ppa_addr</span> <span class="title">cacheline</span>;</span>  <span class="comment">// entry相对于buffer的地址</span></span><br><span class="line">    <span class="keyword">void</span> *data;                 <span class="comment">// 指向数据</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">pblk_w_ctx</span> <span class="title">w_ctx</span>;</span>    <span class="comment">// entry的上下文</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">pblk_w_ctx</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">bio_list</span> <span class="title">bios</span>;</span>       <span class="comment">// 用于回调</span></span><br><span class="line">    u64 lba;                    <span class="comment">// 相对于文件系统的逻辑地址</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ppa_addr</span> <span class="title">ppa</span>;</span>        <span class="comment">// 相对于设备的物理地址</span></span><br><span class="line">    <span class="keyword">int</span> flags;</span><br></pre></td></tr></table></figure></p>
<p>buffer 以 2 的整数次方的大小进行循环管理，buffer 设定的最小值为二进制 110100100，即 420，且向上取 2 的 n 次方整，即 buffer 能取到的最小的大小为 512。实际取值为成功读取所需要的最小距离乘以 * luns 的数量，向上取 2 的 n 次方整。</p>
<hr>
<h2 id="Wear-Leveling"><a href="#Wear-Leveling" class="headerlink" title="Wear-Leveling"></a>Wear-Leveling</h2><p>版本 4.19 中没有见到有关于 wear-leveling 的代码。</p>
<hr>
<h2 id="Lightnvm-驱动相关层"><a href="#Lightnvm-驱动相关层" class="headerlink" title="Lightnvm 驱动相关层"></a>Lightnvm 驱动相关层</h2><p>相较于内核 4.12，当前的 4.19 在 Lightnvm 驱动相关层的源码没有太大变化，源码位于驱动源码文件所在的目录： /drivers/nvme/host/lightnvm.c。在驱动相关层源码中，需要实现下面的操作：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="class"><span class="keyword">struct</span> <span class="title">nvm_dev_ops</span> <span class="title">nvme_nvm_dev_ops</span> = &#123;</span></span><br><span class="line">    .identity          = nvme_nvm_identity,</span><br><span class="line">    .get_l2p_tbl       = nvme_nvm_get_l2p_tbl,</span><br><span class="line">    .get_bb_tbl        = nvme_nvm_get_bb_tbl,</span><br><span class="line">    .set_bb_tbl        = nvme_nvm_set_bb_tbl,</span><br><span class="line">    .submit_io         = nvme_nvm_submit_io,</span><br><span class="line">    .create_dma_pool   = nvme_nvm_create_dma_pool,</span><br><span class="line">    .destroy_dma_pool  = nvme_nvm_destroy_dma_pool,</span><br><span class="line">    .dev_dma_alloc     = nvme_nvm_dev_dma_alloc,</span><br><span class="line">    .dev_dma_free      = nvme_nvm_dev_dma_free,</span><br><span class="line">    .max_phys_sect     = <span class="number">64</span>,</span><br></pre></td></tr></table></figure></p>
<p>前四条操作分别对应 admin IO 的四条命令，即：</p>
<ol>
<li>获取设备的 geometry 信息；</li>
<li>获取 L2P 表；</li>
<li>获取坏块表；</li>
<li>更新坏块表。</li>
</ol>
<p>第五条 submit_io 用于响应 Lightnvm 传来的常规 IO 请求。后面四条是分配和回收内存相关的操作。最后一条 max_phys_sect 是设备支持的最大物理扇区数。</p>
<hr>
<h2 id="IO-命令"><a href="#IO-命令" class="headerlink" title="IO 命令"></a>IO 命令</h2><h4 id="identity"><a href="#identity" class="headerlink" title="identity"></a>identity</h4><p>首先将 Lightnvm request 转换成 NVMe command，然后直接调用 nvme 模块提供的 nvme_submit_sync_cmd 函数（在这其中初始化了一个新的 request），然后执行了这个 request。</p>
<h4 id="get-l2p-tbl"><a href="#get-l2p-tbl" class="headerlink" title="get_l2p_tbl"></a>get_l2p_tbl</h4><p>以 lun 为单位，同 identity 一样，首先要转换成 NVMe command，然后调用 nvme 提供的 nvme_submit_sync_cmd 函数去处理这个请求。如果请求包含了多个 luns，那么对每个 lun 都需要构造 NVMe command 并提交。</p>
<h4 id="get-bb-tbl"><a href="#get-bb-tbl" class="headerlink" title="get_bb_tbl"></a>get_bb_tbl</h4><p>同identity。</p>
<h4 id="set-bb-tbl"><a href="#set-bb-tbl" class="headerlink" title="set_bb_tbl"></a>set_bb_tbl</h4><p>同identity。</p>
<h4 id="read-write"><a href="#read-write" class="headerlink" title="read/write"></a>read/write</h4><p>General IO（除了 Admin 以外的 IO）的处理方式与 Admin IO 的处理方式大同小异，首先要构造 NVMe command，然后根据 command 和 bio 初始化生成 request，最后调用块设备层的 blk_execute_rq_nowait() 函数执行请求。</p>
<hr>
<h2 id="Driver-激活-Lightnvm-驱动相关层"><a href="#Driver-激活-Lightnvm-驱动相关层" class="headerlink" title="Driver 激活 Lightnvm 驱动相关层"></a>Driver 激活 Lightnvm 驱动相关层</h2><ol>
<li><p>注册 lightnvm，在 nvme 初始化时通过调用 nvme_nvm_register() 实现。（/drivers/nvme/host/core.c）</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nvme_alloc_ns()</span><br><span class="line">    <span class="keyword">if</span> (nvme_nvm_ns_supported(ns, id) &amp;&amp; nvme_nvm_register(ns, disk_name, node)) &#123;</span><br><span class="line">        dev_warn(ctrl-&gt;dev, <span class="string">"%s: LightNVM init failure\n"</span>, __func__);</span><br><span class="line">        <span class="keyword">goto</span> out_free_id;</span><br></pre></td></tr></table></figure>
</li>
<li><p>注销 lightnvm，在 nvme 驱动被卸载前通过调用 nvme_nvm_unregister() 实现。（/drivers/nvme/host/core.c）</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nvme_free_ns()</span><br><span class="line"><span class="keyword">if</span> (ns-&gt;ndev)</span><br><span class="line">    nvme_nvm_unregister(ns);</span><br></pre></td></tr></table></figure>
</li>
<li><p>注册 lightnvm_sysfs，在 nvme 初始化时通过调用 nvme_nvm_register_sysfs()。（/drivers/nvme/host/core.c）</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nvme_alloc_ns()</span><br><span class="line">    <span class="keyword">if</span> (ns-&gt;ndev &amp;&amp; nvme_nvm_register_sysfs(ns))</span><br><span class="line">        pr_warn(<span class="string">"%s: failed to register\n"</span>, ns-&gt;disk-&gt;disk_name);</span><br></pre></td></tr></table></figure>
</li>
<li><p>注销 lightnvm_sysfs，在 nvme 驱动被卸载前调用 nvme_nvm_unregister_sysfs()。（/drivers/nvme/host/core.c）</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nvme_ns_remove()</span><br><span class="line">    <span class="keyword">if</span> (ns-&gt;ndev)</span><br><span class="line">        nvme_nvm_unregister_sysfs(ns);</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>（完）</p>

  	</div>

	  <div class="article-tags tags">
		  
	  </div>
	</section>
</article>




      <script>setLoadingBarProgress(60);</script>
    </main>
    
    <footer id="footer" class="clearfix">
  
  

	<div class="social-wrapper">
  	
      
        <a href="mailto:root@xxiong.me" class="social email" target="_blank" rel="external">
          <span class="icon icon-email"></span>
        </a>
      
        <a href="https://github.com/gnekiah" class="social github" target="_blank" rel="external">
          <span class="icon icon-github"></span>
        </a>
      
        <a href="https://twitter.com/gnekiah" class="social twitter" target="_blank" rel="external">
          <span class="icon icon-twitter"></span>
        </a>
      
    
  </div>
  
  <div>Theme <span class="codename">Typescript</span> designed by <a href="http://rakugaki.me/" target="_blank">Art Chen</a>.</div>
  <div>&copy; <a href="/">Gnekiah&#39;s Serenice</a></div>
  
</footer>


    <script>setLoadingBarProgress(80);</script>
    
  </div>

  



<script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
<script>window.jQuery || document.write('<script src="/js/jquery.min.js"><\/script>')</script>

<script src="/js/jquery.fitvids.js"></script>
<script>
	var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
	var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
	var ALGOLIA_API_KEY = "";
	var ALGOLIA_APP_ID = "";
	var ALGOLIA_INDEX_NAME = "";
  var AZURE_SERVICE_NAME = "";
  var AZURE_INDEX_NAME = "";
  var AZURE_QUERY_KEY = "";
  var BAIDU_API_ID = "";
  var SEARCH_SERVICE = "google";
</script>
<script src="/js/search.js"></script>
<script src="/js/app.js"></script>


  <script>setLoadingBarProgress(100);</script>
  
</body>
</html>

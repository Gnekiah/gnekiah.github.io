<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  <title>Gnekiah&#39;s Serenice</title>
  <meta name="description" content="Just write something casually.">
  <meta name="keywords" content="">
  <meta name="HandheldFriendly" content="True">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Just write something casually.">
<meta property="og:type" content="website">
<meta property="og:title" content="Gnekiah&#39;s Serenice">
<meta property="og:url" content="http://spiral.xxiong.me/page/2/index.html">
<meta property="og:site_name" content="Gnekiah&#39;s Serenice">
<meta property="og:description" content="Just write something casually.">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Gnekiah&#39;s Serenice">
<meta name="twitter:description" content="Just write something casually.">
  
  
    <link rel="icon" href="http://www.xxiong.me/favicon.png">
  

	<script src="https://use.typekit.net/eyf3hir.js"></script>
  <script>try{Typekit.load({ async: false });}catch(e){}</script>
  <link rel="stylesheet" href="/style.css">
  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>
  

</head>
</html>
<body>
  
  <div id="loading-bar-wrapper">
  <div id="loading-bar"></div>
</div>

  <script>setLoadingBarProgress(20)</script>
  
  <div id="site-wrapper">
    
    <header id="header">
	<div id="header-wrapper" class="clearfix">
		<a id="logo" href="/">
			<img src="/images/logo.png">
			<span id="site-desc">
			  很随便的，写点儿东西。
      </span>
		</a>
		<button id="site-nav-switch">
	    <span class="icon icon-menu"></span>
	  </button>
	</div>
	<aside id="site-menu">
  	<nav>
  		
        <a href="/" class="nav-index nav">
          首页
        </a>
      
        <a href="/archives" class="nav-archives nav">
          目录
        </a>
      
        <a href="http://blog.xxiong.me/" class="nav-blog nav">
          博客
        </a>
      
        <a href="http://blog.xxiong.me/about.html" class="nav-about nav">
          关于
        </a>
      
        <a href="http://www.xxiong.me" class="nav-home nav">
          主页
        </a>
      
    </nav>
	</aside>
</header>
    <script>setLoadingBarProgress(40);</script>
    
    <main id="main" role="main">
      





<section class="post-list">
	
    <article class="post ">

  
  <h2 class="title">
    <a href="/2018/12/05/LightNVM-pblk-source-based-on-linux-4-12-rc2/">
      LightNVM - pblk 源码解析（基于 linux-4.12-rc2）
    </a>
  </h2>
  
  <time>
    Dec 5, 2018
  </time>
  <section class="content">
	  <h2 id="基本结构图"><a href="#基本结构图" class="headerlink" title="基本结构图"></a>基本结构图</h2><p>下图是 pblk 的结构图，其中给出了 read 请求和 write 请求的操作示意图，以及 pblk 核心的几个数据结构——L2P table、write buffer、write context。关于具体操作的细节我们在后面的小节给出：<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/2440312904.png" alt="20181130202318.png"><br>!!!<br></center><br>!!!</p>
<ol>
<li>Buffer 包含两块内容：write buffer 和 write context。write buffer 存储 4KB 为单位(sector)的 data entries，其中write buffer 的大小为 page 的大小 * PUs 的数量 * 8；write context 存储每个 entry 的 metadata。</li>
<li>L2P table 是 target 的逻辑地址到物理地址的映射，这是 pblk 中的映射，在请求下发给 nvm manager 后 target 的物理地址会再被映射为 device 的物理地址。</li>
<li>write thread 是一个单独的线程，在 lightnvm 中，write buffer 有多个“Producers”和 一个“Consumer”，其中的 Consumer 就是这里的 write thread。write thread 在两种情况下会被唤醒：<ul>
<li>定时器触发；</li>
<li>write buffer 达到 flush 的条件(主动唤醒);</li>
</ul>
</li>
</ol>
<hr>
<h2 id="line-的概念"><a href="#line-的概念" class="headerlink" title="line 的概念"></a>line 的概念</h2><p>line：从 target 的每个 lun 中取一个 block，组成一个 line。下图是一个例子，帮助我们直观的认识 line。在这个 target 中有 4 个 luns，每个 lun 中有 6 个 blocks，在这个例子中，一共有 6 个 lines，每个 line 的大小是 4 个 blocks：<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/3321758576.png" alt="20181130202319.png"><br>!!!<br></center><br>!!!</p>
<ul>
<li>line 0 包含的 blocks 编号为：0，6，12，18</li>
<li>line 1 包含的 blocks 编号为：1，7，13，19</li>
<li>………………………………</li>
</ul>
<hr>
<h2 id="discard-请求"><a href="#discard-请求" class="headerlink" title="discard 请求"></a>discard 请求</h2><p>discard 的作用是使请求的数据无效化。discard 是针对 L2P table 的操作，只需要将 L2P table 的逻辑地址对应的物理地址设为 empty 就实现了将目标数据无效化的操作。涉及到 discard 的操作如下：<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/1260758134.png" alt="20181130202320.png"><br>!!!<br></center><br>!!!<br>当 discard 请求发送到来时，首先查找 L2P table，找到对应的物理地址；之后查找 write buffer，如果 cache hit，则直接更新 L2P table 将请求的逻辑地址对应的物理地址标记为无效；如果 cache miss，则获取请求页地址所在的 line，将其标记为不可用（确保在该地址被无效化之前不会被访问），之后更新 L2P table。相关 discard 的操作流程见下方的流程图：<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/3492987467.png" alt="20181130202321.png"><br>!!!<br></center><br>!!!</p>
<hr>
<h2 id="read-请求"><a href="#read-请求" class="headerlink" title="read 请求"></a>read 请求</h2><p>read 操作大致可分为三步：</p>
<ol>
<li>从 L2P table 查找物理地址；</li>
<li>cache hit，则从 buffer 中读取；</li>
<li>cache miss，则从设备读取；</li>
</ol>
<p>注意：如果只有部分请求 cache hit，则要将 cache miss 的请求重新构造成新的请求，再从设备读取。在这种情况下，当请求从设备中读取数据后，要将 new  bio 中的数据拷贝到 original bio 中。（所有请求都 cache miss 的情况下，从设备中读取数据后不执行该步骤。）<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/1384739315.png" alt="20181130202322.png"><br>!!!<br></center><br>!!!</p>
<p>下面的流程图展示了整个 read 操作的过程。对于到达的 read 请求，首先查找 L2P table，获取该请求的物理地址；然后查找 write buffer，当 cache hit 的情况下，将 write buffer 中的数据拷贝到 bio 中。这里牵涉到两种情况：只有部分请求页 cache hit 和 所有请求页都 cache hit。如果是所有请求页都 cache hit，则 end io；如果只有部分请求页 cache hit，那么需要将 cache miss 的请求页重新狗造成一个新的请求，然后下发给设备用于读取数据。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/2653169438.png" alt="20181130202323.png"><br>!!!<br></center><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/3328136321.png" alt="20181130202324.png"><br>!!!<br></center><br>!!!<br>在向设备发送请求时，如果是请求页全部 cache miss 的情况，则直接提交请求；如果是部分 cache miss 的情况，则需要先将 cache miss 的请求页拼接成一个新的请求，然后下发给设备，直到设备读取数据并返回后，需要将上一步构造的新的 bio 中的数据拷贝到原始请求的 bio 中。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/2697667031.jpg" alt="20181130202318.jpg"><br>!!!<br></center><br>!!!</p>
<hr>
<h2 id="write-请求"><a href="#write-请求" class="headerlink" title="write 请求"></a>write 请求</h2><p>write 操作分为两个部分：</p>
<ol>
<li>写入 write buffer；</li>
<li>write thread从 write buffer 写入设备；</li>
</ol>
<p>在整个写入的操作中，对buffer的操作可以分为两类角色：多个生产者和一个消费者。所有的写操作都会将数据写入 write buffer（扮演生产者），然后由write thread将数据写入设备中（扮演消费者）。下面是这两个部分的具体操作流程：<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/2557055257.png" alt="20181130202325.png"><br>!!!<br></center><br>!!!</p>
<p>写入 buffer 的主要操作如下：</p>
<ol>
<li>判断能否向 buffer 写入请求的 entries；</li>
<li>向 buffer 中写入 entries；</li>
<li>更新 L2P table；</li>
<li>判断是否需要唤醒 write thread；</li>
</ol>
<p>下面的流程图展示了向buffer中写数据的操作过程。当write请求到来时，首先要判断有没有write buffer足够空间用来进行本次的写请求，如果空间不够，需要进行io调度，将buffer中的数据写回设备中。如果空间足够，就将数据写入到write buffer中，并更新write context，其中write buffer中的单位是entry，大小是4KB。最后再更新 L2P table。这里在end io之前需要判断是否需要唤醒write thread将buffer中的数据写入到设备。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/1387548915.png" alt="20181130202326.png"><br>!!!<br></center><br>!!!</p>
<p>从 buffer 写入设备的主要操作：</p>
<ol>
<li>计算要写回的 entries 数量；</li>
<li>将 entries 添加到 bio 构造 write request；</li>
<li>nvme submit io向设备提交请求；</li>
</ol>
<p>下图是write thread进行的操作流程：<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/2770234034.png" alt="20181130202327.png"><br>!!!<br></center><br>!!!</p>
<p>write thread 被唤醒的条件有两个：</p>
<ol>
<li>被定时器触发；</li>
<li>write buffer 中需要写回到设备的数据量达到了阈值；</li>
</ol>
<p>当 write thread 被唤醒后，首先计算 write buffer 中需要同步的 entries 的总数，这些是需要写回设备的数据单元。之后将 entries 中的数据添加到 bio，用于向设备发送写请求。这里需要注意，write thread 不需要更新 L2P table，因为这个操作在前半部分的 write to buffer 中已经完成了。</p>
<hr>
<h2 id="GC"><a href="#GC" class="headerlink" title="GC"></a>GC</h2><p>GC 能够充分将存储单元利用起来。GC thread 被唤醒的条件有：</p>
<ol>
<li>定时器唤醒；</li>
<li>write thread 主动唤醒 GC thread；<br>!!!<br><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/3960561637.png" alt="20181130202328.png"><br>!!!<br></center><br>!!!</li>
</ol>
<p>在GC thread被唤醒后，只遍历每个line，并初始化每个line 的工作队列。如上图所示，gc full list 中保存的是 lines，只有全部 full 的 line 才会加入 gc full list。停止 gc 的条件是 free blocks 数量达到预设的阈值。GC是由line管理的，内核遍历由line中的工作队列组成的工作队列链表，对每一个line，分别执行GC 操作。在这个版本的lightnvm中，GC的操作是简单的将数据读出来保存到write buffer中。关于擦除块的操作在write thread中。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/649340819.png" alt="20181130202329.png"><br>!!!<br></center><br>!!!</p>
<hr>
<h2 id="L2P-table-recovery"><a href="#L2P-table-recovery" class="headerlink" title="L2P table recovery"></a>L2P table recovery</h2><p>L2P table 是 lightnvm 一个非常核心的数据结构，这张表建立起从逻辑地址到存储器物理地址的映射关系，因此该表是保存在设备的 oob 部分的，当设备初始化的时候直接向设备发送读取 l2p 的请求，即可读取 L2P 表。</p>
<p>（完）</p>


    
    
    

  </section>
</article>
  
    <article class="post ">

  
  <h2 class="title">
    <a href="/2018/12/05/LightNVM/">
      LightNVM 简介
    </a>
  </h2>
  
  <time>
    Dec 5, 2018
  </time>
  <section class="content">
	  <p>LightNVM<a href="https://github.com/OpenChannelSSD/linux" target="_blank" rel="noopener">1</a> 是 CNEXLabs 针对 Open Channel SSD（以下简称 OCSSD）在 Linux 内核中的一种实现，分支托管在 <a href="https://github.com/OpenChannelSSD/linux" target="_blank" rel="noopener">GitHub</a> 上，目前能找到最早的提交记录是 2015-10-29。LightNVM 的程序栈由三层组成，每一层都向用户空间提供了 OCSSD 的抽象（即用户可以直接与这三层进行 IO 交互而不用经过文件系统，后面会细提到）。示意图如下图所示：<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/133890102.jpg" alt="20181130202312.jpg"><br>!!!<br></center><br>!!!</p>
<ul>
<li>最上层(3)是对 FTL 的具体实现，其中包含了基本的地址转换、GC 等操作实现，向上呈现为一个逻辑块设备；</li>
<li>中间的 LightNVM 子系统(2)管理整块设备的划分与聚合，向上可以将多个物理设备以一个逻辑设备或一个物理设备划分为多个逻辑设备的形式提供给 FTL；</li>
<li>最后是与驱动程序的对接(3)，我们称为 LightNVM Lower-Level（驱动相关层），用于实现 LightNVM 的命令到具体设备驱动命令的转换。在当前版本（linux-4.12-rc2）中只实现了与 NVMe 驱动的对接。</li>
</ul>
<hr>
<h2 id="LightNVM-Lower-Level"><a href="#LightNVM-Lower-Level" class="headerlink" title="LightNVM Lower-Level"></a>LightNVM Lower-Level</h2><p>启用了 LightNVM 的 NVMe 设备驱动程序使内核模块能够通过 PPA I/O 接口访问 OCSSD。设备驱动程序将设备作为传统的 Linux 块设备呈现到用户空间，允许应用程序通过 ioctls 与设备进行交互。如果 PPA 接口通过 LBA 公开，那么它也可能会相应地发出 I/O。其中，PPA（Physical Page Address）是一种专为 OCSSD 设计的 I/O 接口，它定义了管理级命令将设备的几何信息（channel, die, plane 等）并且让主机端能对 SSD 进行管理，下图是一个是传统逻辑块地址与其进行的对比：<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/174123964.jpg" alt="20181130202313.jpg"><br>!!!<br></center><br>!!!<br>由图中可以看出，逻辑地址是一维的，每个地址对应到存储设备的一个 sector；而 PPA 地址则为每个段赋予了不同的涵义，地址最前端是 Channel 的地址，通常 SSD 设备具有数十个到数百个不等的 Channels，PPA 地址的 Channel 段指定了请求要访问的 Channel 地址；以此类推，后面分别是芯片地址 PU、片内的块地址 Block、分层地址 Plane、块内的页地址 Page 以及 Sector。OCSSD 设备接收到 PPA 格式的地址后，无需进行映射，直接根据这个地址就能定位到唯一的 Page 或 Block。PPA 地址中每个字段的宽度不定，通常是在设备加载时，通过获取设备几何结构信息后再计算的，这样能够增加灵活性。</p>
<hr>
<h2 id="LightNVM-Subsystem"><a href="#LightNVM-Subsystem" class="headerlink" title="LightNVM Subsystem"></a>LightNVM Subsystem</h2><p>LightNVM 子系统对应到具体的物理设备，为每个设备创建一个实例。其实例在 PPA I/O 接口支持的块设备的基础上初始化。该实例使内核能够通过内部 nvm_dev 数据结构和 sysfs 等来暴露设备的几何结构信息，几何结构信息就是类似上面 PPA 地址中所涉及的信息，例如设备包含多少 channels、channel 中包含多少 PUs、有多少个 blocks 等。这样，FTL 和用户空间的应用程序可以在使用前就了解到设备的底层结构信息。它还使用 blk-mq<a href="http://blog.xxiong.me/usr/uploads/2018/11/133890102.jpg" target="_blank" rel="noopener">2</a> 设备驱动程序专用的 I/O 接口公开 vector IO 接口，使得应用程序能够通过设备驱动程序有效地下发 vector I/O。</p>
<hr>
<h2 id="pblk"><a href="#pblk" class="headerlink" title="pblk"></a>pblk</h2><p>物理块设备（pblk）是 LightNVM 中最重要的一个部分，为上层的 target 抽象实现了完全关联的基于主机的 FTL 功能，为上层暴露出了传统块设备的 I/O 接口。其中，每一个 target 就是物理存储设备的逻辑抽象，彼此间独立，由 LightNVM 子系统划分与管理。target 这样的抽象使内核空间模块或用户空间应用程序能够通过高级 I/O 接口（例如由 pblk 提供的 Block I/O 接口的标准接口访问 OCSSD，或由自定义 target 提供的为应用程序定制的接口来进行访问。主要的职责是以下几点：</p>
<ul>
<li>处理控制器和介质媒体的限制（写缓冲等）；</li>
<li>逻辑地址到物理地址的转换；</li>
<li>错误处理；</li>
<li>垃圾回收；</li>
</ul>
<p>具体的实现细节已经在论文中体现（<a href="https://www.usenix.org/conference/fast17/technical-sessions/presentation/bjorling" target="_blank" rel="noopener">LightNVM: The Linux Open-Channel SSD Subsystem</a>），<a href="https://openchannelssd.readthedocs.io/en/latest/``" target="_blank" rel="noopener">官方文档</a>也一直在持续更新。</p>
<p>（完）</p>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://github.com/OpenChannelSSD/linux" target="_blank" rel="noopener">1</a> Bjørling M, González J, Bonnet P. LightNVM: The Linux Open-Channel SSD Subsystem[C]//FAST. 2017: 359-374.<br><a href="http://blog.xxiong.me/usr/uploads/2018/11/133890102.jpg" target="_blank" rel="noopener">2</a> Bjørling M, Axboe J, Nellans D, et al. Linux block IO: introducing multi-queue SSD access on multi-core systems[C]//Proceedings of the 6th international systems and storage conference. ACM, 2013: 22.</p>


    
    
    

  </section>
</article>
  
    <article class="post ">

  
  <h2 class="title">
    <a href="/2018/12/05/Open-Channel-SSD/">
      Open-Channel SSD 简介
    </a>
  </h2>
  
  <time>
    Dec 5, 2018
  </time>
  <section class="content">
	  <h2 id="SSD-的特点"><a href="#SSD-的特点" class="headerlink" title="SSD 的特点"></a>SSD 的特点</h2><p>SSD 设备的存储单元主要是 <a href="https://link.springer.com/content/pdf/10.1007/978-90-481-9431-5.pdf" target="_blank" rel="noopener">NAND Flash</a><a href="https://link.springer.com/content/pdf/10.1007/978-90-481-9431-5.pdf" target="_blank" rel="noopener">1</a>，按 page 写入，按 block 擦除，一个 block 内有多个 page，并且擦除的次数有限。根据以上的特点，在使用设备时，必然存在这样的情况：在一个 block 中，有部分 page 保存了有效数据，剩下的 page 全被标记为无效，如果要擦除这个 block，就必须首先将其中的有效数据转移到其他 block，然后才能擦除当前 block，这个过程被称为 GC。</p>
<p>在写入数据时，有些区域保存了热数据，对应的 block 会被频繁擦除，达到一定次数后就无法再写入数据，将被标记为坏块；有的区域保存了冷数据，例如备份的资源文件等，这样就会造成磨损的失衡。极端的例子：一块 SSD 其中一半区域被频繁擦除，导致全部被标记为坏块，寿命归零；而另一半的区域由于不常访问，寿命接近满编。如果能够平摊寿命，那么这个 SSD 就能使用更久，平摊寿命这一过程被称为 WL（磨损均衡）。</p>
<p>根据 IO 栈的分层设计思想，同时考虑到设备厂商的各种质保承诺，以及兼容现有的 IO 协议和接口，SSD 被设计成如下图所示的结构：<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/1109110897.jpg" alt="20181130162926.jpg"><br>!!!<br></center><br>!!!<br>系统驱动与 SSD 控制芯片之间传输的 IO 请求使用的是逻辑地址，需要经过控制芯片中的 FTL（Flash Translation Layer）转换成真实地址，这样就完成了逻辑地址到真实地址的映射。FTL 负责的主要功能有：磨损均衡、地址映射、坏块管理等，可以在一定程度上提高使用寿命，但是也存在负面的影响，例如：</p>
<ul>
<li>数据迁移、调度、垃圾回收、磨损均衡等都由固件决定, 一旦定制后难以修改，缺少灵活性；</li>
<li>固件只能通过对工作负载的分析进行设计，通用的 SSD 通常只能采取折中的设计方案；</li>
<li>即使大厂可以进行专用 SSD 的定制，工作负载变更将导致定制的 FTL 发挥不出性能；</li>
<li>GC、WL、over-provisioning 等操作在一定程度上浪费了 SSD 的存储空间和使用寿命；</li>
<li>由于 FTL 置于“黑匣子”中，无法保证可以预测的请求时延（latency）；</li>
</ul>
<hr>
<h2 id="SSD-的发展趋势"><a href="#SSD-的发展趋势" class="headerlink" title="SSD 的发展趋势"></a>SSD 的发展趋势</h2><p>SSD 由于其优异的存储性能，几乎已成为云平台的标配；在个人市场领域，SSD 也同样被广泛地使用。我们知道 SSD 控制器内部算法核心是 FTL，把主机侧逻辑地址转换为 SSD 内部 NAND 芯片的物理地址。一般的消费级 SSD 控制器内置 FTL，因为功能比较简单和统一，对性能的需求也较低，且消费级市场经过 WinTel 联盟多年的锤炼，各种接口非常统一，用户的需求也很单一，只要支持 Intel 主板、Windows 操作系统就可以了，大大简化了各种外设的硬件设计。</p>
<p>但是进入了群雄并立的企业级市场，有各种各样的客户和芯片厂家，大家使用多种多样的操作系统和架构，甚至 Google、Facebook、BAT 等大厂都开始定制硬件、修改内核以支持自身的需求。在这种情况下，FTL 放在 SSD 控制器里面已经难以满足需求，企业用户希望能够根据自己的数据特点设计高效的 FTL，例如：</p>
<ul>
<li>搜索引擎可以把索引表和 SSD 物理地址对应起来；</li>
<li>日志数据可以直接流式写入 Flash 通道；</li>
<li>数据库希望 key-value 能对应到 SSD 物理地址；</li>
</ul>
<hr>
<h2 id="Open-Channel-SSD"><a href="#Open-Channel-SSD" class="headerlink" title="Open Channel SSD"></a>Open Channel SSD</h2><p><a href="https://events.static.linuxfound.org/sites/events/files/slides/LightNVM-Vault2015.pdf" target="_blank" rel="noopener">Open Channel SSD</a> 是面向企业级市场的其中一个发展方向。在 FAST’2015 闪存峰会上，CNEXLabs 介绍了他们的 Open Channel SSD<a href="http://blog.xxiong.me/usr/uploads/2018/11/1109110897.jpg" target="_blank" rel="noopener">2</a> 概念，通过将 FTL 的功能移动到主机侧的方式，实现一定程度的 FTL 定制，给企业级 SSD 应用提供了新的解决方案。基本架构如下图所示：</p>
<p>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/11/3099188414.jpg" alt="20181130193321.jpg"><br>!!!<br></center><br>!!!</p>
<p>该方案把 SSD 内部的存储通道开放给主机系统，这样控制器只负责 Flash 数据传输、ECC、错误处理、坏块管理等工作，而 FTL 层的设计由用户自己根据需求实现。这样也更加方便以更高效的方式控制 SSD 阵列，实现了在软件中定义存储。通过 CNEXLabs 公布的实验数据得知，主机侧增加的任务，造成的 latency 只增加了很少的一部分。上图中的 LightNVM<a href="https://events.static.linuxfound.org/sites/events/files/slides/LightNVM-Vault2015.pdf" target="_blank" rel="noopener">3</a> 架构是 CNEXLabs 提供的对 Open-Channel 的一种实现。通过把 SSD 中的 FTL 层集成到主机系统中，对工作负载的优化工作就可以在驱动层、块层、文件系统或应用程序本身中进行。</p>
<hr>
<h2 id="Open-Channel-SSD-的优势"><a href="#Open-Channel-SSD-的优势" class="headerlink" title="Open Channel SSD 的优势"></a>Open Channel SSD 的优势</h2><p>根据 CNEXLabs 官方的数据，目前 OCSSD 已经被 WEB 量级的数据中心、超融合基础设施（Hyper-converged Infrastructure）、Flash Array Vendors、高性能计算中心、Tier1 云服务提供商等采用。例如百度使用 OCSSD 来简化一个键值存储的存储堆栈；Fusion-IO 和 Violin 内存均实现主机端存储堆栈来管理 NAND 介质并提供 Block I/O 接口。如下列举一些 OCSSD 的优势：</p>
<ul>
<li><p>可预测性</p>
<ul>
<li>达到一致且可预测的 IO，低时延的 QoS；</li>
<li>减少了写放大问题；</li>
<li>能够实现 IO 隔离，减少了邻间干扰的问题；</li>
</ul>
</li>
<li><p>方便管理</p>
<ul>
<li>能够全局地管理所有的 Flash 存储介质；</li>
<li>PB 级存储的实现；</li>
<li>单层的地址空间；</li>
<li>根据应用工作负载的特点来配置所期望的 FTL 功能从而更好地优化 IO；</li>
</ul>
</li>
</ul>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>OCSSD 与传统的消费级 SSD 在 IO 通路上的区别就在于：</p>
<ol>
<li>对于传统 SSD 而言，请求从文件系统经过调度最终被驱动程序发出来时，请求中指向的地址是逻辑地址，在 SSD 内部的控制芯片中，被转换成了物理地址，负责这个工作的程序被称为 FTL。<br>!!!<br><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/4056922680.jpg" alt="20181130202317.jpg"><br>!!!<br></center><br>!!!</li>
<li>对于 OCSSD，请求从文件系统发下来后，首先被转换成了物理地址（或者文件系统发出来就直接就是物理地址），然后再由驱动程序发给 OCSSD 控制器，控制器接收到的请求中的地址是物理地址。因此也被称为 FTL 上移到系统端。</li>
</ol>
<p>（完）</p>
<hr>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://link.springer.com/content/pdf/10.1007/978-90-481-9431-5.pdf" target="_blank" rel="noopener">1</a> Micheloni R, Crippa L, Marelli A. Inside NAND Flash Memories[J]. 2010.<br><a href="http://blog.xxiong.me/usr/uploads/2018/11/1109110897.jpg" target="_blank" rel="noopener">2</a> Bjørling M. Open-Channel Solid State Drives[J]. Vault, Mar, 2015, 12: 22.<br><a href="https://events.static.linuxfound.org/sites/events/files/slides/LightNVM-Vault2015.pdf" target="_blank" rel="noopener">3</a> Bjørling M, González J, Bonnet P. LightNVM: The Linux Open-Channel SSD Subsystem[C]//FAST. 2017: 359-374.</p>


    
    
    

  </section>
</article>
  
    <article class="post ">

  
  <h2 class="title">
    <a href="/2018/12/05/LightNVM-test-environment-build/">
      LightNVM 测试环境搭建
    </a>
  </h2>
  
  <time>
    Dec 5, 2018
  </time>
  <section class="content">
	  <p>要使用 Open-Channel SSD，需要得到操作系统内核支持。 随着 LightNVM 的加入，4.4 版本后的 linux 内核都可以支持。该项目仍然处于开发中，最新的源代码可从 <a href="https://github.com/OpenChannelSSD/linux" target="_blank" rel="noopener">https://github.com/OpenChannelSSD/linux</a> 获得。启用了相应的内核支持后，必须满足以下条件：</p>
<ul>
<li>兼容的设备，如 QEMU NVMe 或 Open-Channel SSD，如 CNEXLabs LightNVM SDK；</li>
<li>设备驱动程序顶部的媒体管理器。 媒体管理器管理设备的分区表；</li>
<li>在 Block Manager 上层暴露出 Open-Channel SSD 的目标（target）；</li>
</ul>
<hr>
<h2 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h2><p>目前由于 CNEXLabs 的 Open-Channel SSD 还没有购置。因此利用 Qemu 虚拟机模拟 Open-Channel SSD 设备。注意 Qemu 必须能够支持 Open-Channel，可以使用 CNEXLabs 提供的 <a href="https://github.com/OpenChannelSSD/qemu-nvme" target="_blank" rel="noopener">Qemu-nvme 分支</a>。依照官方文档进行配置和编译后，才可以通过后端文件暴露出一个 LightNVM 兼容的设备。当完成安装，内核也被顺利启动后，设备就能够被观察到，并且可以通过 nvme-cli 的工具进行管理和初始化。实验内核：linux-4.12-rc2。</p>
<hr>
<h2 id="实验设计"><a href="#实验设计" class="headerlink" title="实验设计"></a>实验设计</h2><h4 id="lightnvm-hello-world"><a href="#lightnvm-hello-world" class="headerlink" title="lightnvm hello world"></a>lightnvm hello world</h4><p>环境配置成功后，可以首先重复一下论文中的一些实验进行验证，观察和分析 OCSSD 的特点，熟悉 lightnvm 的使用。</p>
<h4 id="多租户应用实验"><a href="#多租户应用实验" class="headerlink" title="多租户应用实验"></a>多租户应用实验</h4><p>利用 OCSSD 来实现多租户的应用，每一个 target 对应一个租户，以同样的 workload，与传统的 SSD 的数据进行对比，分析结果。改变 workload 后再同样进行分析，发现 lightnvm 的特点和不足。</p>
<h4 id="算法改进"><a href="#算法改进" class="headerlink" title="算法改进"></a>算法改进</h4><p>由于 lightnvm 把 FTL 的功能等都提高到上层，数据迁移，GC 等算法都能够看到如何被实现，也意味着可以进行优化，并且编译自己改进的内核，从而有针对性地提高性能。</p>
<hr>
<h2 id="配置环境参考"><a href="#配置环境参考" class="headerlink" title="配置环境参考"></a>配置环境参考</h2><p>下面介绍目前我们能成功跑通 LightNVM 例程的环境搭建流程，主机端安装的是 Ubuntu 17.04 发行版，内核版本为 4.10，提供编译内核的环境，还存在一些不便和不足，后期继续优化：</p>
<h4 id="内核版本-linux-4-12-rc2-的编译"><a href="#内核版本-linux-4-12-rc2-的编译" class="headerlink" title="内核版本 linux-4.12-rc2 的编译"></a>内核版本 linux-4.12-rc2 的编译</h4><ul>
<li><p>从 GitHub 下载 <a href="http://github.com/OpenChannelSSD/linux" target="_blank" rel="noopener">master 分支</a>；</p>
</li>
<li><p>编译内核，需要注意配置文件中，应该确定以下被配置：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CONFIG_NVM=y </span><br><span class="line">CONFIG_NVM_DEBUG=y </span><br><span class="line">CONFIG_NVM_PBLK=y </span><br><span class="line">CONFIG_BLK_DEV_NVME=y</span><br></pre></td></tr></table></figure>
</li>
<li><p>在 arch/x86/boot/ 目录下，会生成编译好后的 bzImage 文件，大小 7M 左右；</p>
</li>
</ul>
<h4 id="Qemu的编译"><a href="#Qemu的编译" class="headerlink" title="Qemu的编译"></a>Qemu的编译</h4><ul>
<li><p>从 GitHub 下载 <a href="https://github.com/OpenChannelSSD/qemu-nvme.git" target="_blank" rel="noopener">Qemu 主分支</a>；</p>
</li>
<li><p>运行如下的配置命令（官方文档中也有详细的介绍）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./configure --<span class="built_in">enable</span>-linux-aio --target-list=x86_64-softmmu --<span class="built_in">enable</span>-kvm</span><br><span class="line">modprobe kvm-intel <span class="comment"># 安装 kvm 模块</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>编译的过程中可能需要安装一些库</p>
</li>
</ul>
<h4 id="Qemu-运行虚拟机"><a href="#Qemu-运行虚拟机" class="headerlink" title="Qemu 运行虚拟机"></a>Qemu 运行虚拟机</h4><ul>
<li><p>创建一个空的文件来模拟 nvme 的设备：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dd <span class="keyword">if</span>=/dev/zero of=blknvme bs=1M count=1024</span><br></pre></td></tr></table></figure>
</li>
<li><p>用以下命令启动一个预装的 linux 镜像作为 Qemu 的运行环境：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">qemu-system-x86_64 -m 4G -smp 1 --<span class="built_in">enable</span>-kvm -hda <span class="variable">$LINUXVMFILE</span> -append \</span><br><span class="line">    <span class="string">"root=/dev/sda1"</span> -kernel <span class="string">"/home/foobar/git/linux/arch/x86_64/boot/bzImage"</span> \</span><br><span class="line">    -drive file=blknvme, <span class="keyword">if</span>=none, id=mynvme -device nvme, drive=mynvme, \</span><br><span class="line">    serial=deadbeef, namespaces=1, lver=1, nlbaf=5, lba_index=3, mdts=10</span><br></pre></td></tr></table></figure>
</li>
<li><p>其中，用预装好的镜像 .img 文件替换 $LINUXVMFILE，我们采用的是 Ubuntu 16.04 的系统。可以在 Qemu 里安装，也可以在 virtual-box 等虚拟机中装好，再通过下面的命令加载：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qemu-img convert -f vmdk -O raw ubuntu.vmdk image.img</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="Hello-world"><a href="#Hello-world" class="headerlink" title="Hello world"></a>Hello world</h4><ul>
<li><p>安装 nvme-cli 工具，通过 apt install 或者从 <a href="https://github.com/linux-nvme/nvme-cli" target="_blank" rel="noopener">GitHub</a> 下载；</p>
</li>
<li><p>一切就绪后，就可以通过 sudo nvme lnvm list 命令来列出设备；</p>
</li>
</ul>
<p>（完）</p>


    
    
    

  </section>
</article>
  
    <article class="post ">

  
  <h2 class="title">
    <a href="/2018/12/05/LightNVM-Open-Channel-SSD-connection-IO-stack-position/">
      LightNVM 与 Open Channel SSD 的关系以及在 IO 栈上的位置
    </a>
  </h2>
  
  <time>
    Dec 5, 2018
  </time>
  <section class="content">
	  <p>Open-Channel SSD 是一种设备，与 SSD 不同之处在于，前者将 SSD 的 FTL(Flash Translation Layer) 提出来，交给主机管理与维护，其优点是：高吞吐，低延迟，高并行。LightNVM 则是 Open-Channel SSD 在主机上的驱动程序扩展。</p>
<hr>
<h2 id="OCSSD-的特性"><a href="#OCSSD-的特性" class="headerlink" title="OCSSD 的特性"></a>OCSSD 的特性</h2><ol>
<li>I/O 分离：将 SSD 划分为数个 channels, 映射到设备的并行单元上。应用举例：多个应用程序能够同时访问不同的 channels 来实现并行的进行 I/O 操作。</li>
<li>可预测的延迟：通过控制主机何时、向何地址、如何提交 I/O 给 OCSSD 来实现可预测的延迟。</li>
<li>软件定义存储：通过将 SSD 的 FTL 集成到主机中，能够实现根据实际应用的特点，在主机 FTL 中进行负载优化，或者在文件系统中优化，甚至也能够在应用程序中实现。</li>
</ol>
<hr>
<h2 id="LightNVM-在-IO-栈中所处的位置"><a href="#LightNVM-在-IO-栈中所处的位置" class="headerlink" title="LightNVM 在 IO 栈中所处的位置"></a>LightNVM 在 IO 栈中所处的位置</h2><p>下图是 LightNVM 的分层结构示意图，直观的来说，OCSSD 即是设备，主机系统通过 NVMe 设备驱动程序与 OCSSD 进行数据的交换。传统的 SSD 通常走 SCSI 驱动，其之上就是 block 层以及文件系统，或者是 NVMe 驱动，直接处理 bio；而 OCSSD 的设备由于已经将 FTL 的功能挪到了主机侧，所以直接接受的请求是物理地址的，因此在驱动程序之上，必须要有一个实现地址转换的程序。图中的 FTL（Block Device Target 和 General Media Manager）就是这个功能的实现。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/3566424873.jpg" alt="20181130202314.jpg"><br>!!!<br></center><br>!!!<br>用户空间的应用程序可以通过文件系统访问 OCSSD，在这条通路上，文件系统下发的 bio 将直接被 LightNVM 打包成 nvmrq 的请求格式，这是 LightNVM 定义的请求格式；之后 nvmrq 被下发给 LightNVM 的驱动相关层，驱动相关层实现 nvmrq 到具体驱动程序定义的请求格式的转换。如下图所示，驱动相关层由具体驱动程序加载，当设备支持 OCSSD 时，将自动激活驱动相关层。目前的版本中只支持 NVMe。<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/816566981.jpg" alt="20181130202315.jpg"><br>!!!<br></center><br>!!!<br>这样设计的好处是：可以很方便的将 LightNVM 移植到使用其他协议的设备上，例如 UFS 或者 SATA，只需要满足设备为 Open Channel 的设备，然后在驱动中为 LightNVM 的激活和命令转换提供支持即可。</p>
<hr>
<h2 id="LightNVM-的源码结构（linux-4-10-3）"><a href="#LightNVM-的源码结构（linux-4-10-3）" class="headerlink" title="LightNVM 的源码结构（linux-4.10.3）"></a>LightNVM 的源码结构（linux-4.10.3）</h2><ul>
<li>实现 FTL：rrpc.[ch] (round robin, page-based FTL, and cost-based GC)</li>
<li>General Media Manager：gennvm.[ch]</li>
<li>lightnvm core：core.c、include/linux/lightnvm.h</li>
<li>NVMe 被扩展用于向 LightNVM 设备提供支持：drivers/nvme/host/lightnvm.c </li>
</ul>
<p>LightNVM 的核心数据结构如下图所示：<br>!!!</p>
<p><center><br>!!!<br><img src="http://blog.xxiong.me/usr/uploads/2018/12/2329710792.jpg" alt="20181130202316.jpg"><br>!!!<br></center><br>!!!<br>每块物理的 OCSSD 都会通过 NVMe 驱动程序激活设备相关层，并在 General Media Manager 中为其创建一个 nvm_dev 的结构体实例和 gen_dev 实例，代表一块完整的设备；nvm_dev 可以被划分为多个 target，类似于磁盘分区，不同之处在于，LightNVM 只能按照 luns 为基本单位进行划分。每个 target 对应到一个 nvm_tgt_dev 的结构体，以及用于处理这个分区的负责实现 FTL 功能的 target type，目前实现的 type 只有 rrpc 一种。</p>
<p>（完）</p>


    
    
    

  </section>
</article>
  
</section>


  <nav id="page-nav">
    
    <a class="prev" rel="prev" href="/">
      <span class="icon icon-chevron-left"></span>
      <span class="text">Previous</span>
    </a>
    
    
  </nav>
  

      <script>setLoadingBarProgress(60);</script>
    </main>
    
    <footer id="footer" class="clearfix">
  
  

	<div class="social-wrapper">
  	
      
        <a href="mailto:root@xxiong.me" class="social email" target="_blank" rel="external">
          <span class="icon icon-email"></span>
        </a>
      
        <a href="https://github.com/gnekiah" class="social github" target="_blank" rel="external">
          <span class="icon icon-github"></span>
        </a>
      
        <a href="https://twitter.com/gnekiah" class="social twitter" target="_blank" rel="external">
          <span class="icon icon-twitter"></span>
        </a>
      
    
  </div>
  
  <div>Theme <span class="codename">Typescript</span> designed by <a href="http://rakugaki.me/" target="_blank">Art Chen</a>.</div>
  <div>&copy; <a href="/">Gnekiah&#39;s Serenice</a></div>
  
</footer>


    <script>setLoadingBarProgress(80);</script>
    
  </div>

  



<script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>
<script>window.jQuery || document.write('<script src="/js/jquery.min.js"><\/script>')</script>

<script src="/js/jquery.fitvids.js"></script>
<script>
	var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
	var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
	var ALGOLIA_API_KEY = "";
	var ALGOLIA_APP_ID = "";
	var ALGOLIA_INDEX_NAME = "";
  var AZURE_SERVICE_NAME = "";
  var AZURE_INDEX_NAME = "";
  var AZURE_QUERY_KEY = "";
  var BAIDU_API_ID = "";
  var SEARCH_SERVICE = "google";
</script>
<script src="/js/search.js"></script>
<script src="/js/app.js"></script>


  <script>setLoadingBarProgress(100);</script>
  
</body>
</html>
